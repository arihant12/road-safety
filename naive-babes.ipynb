{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5793796,"sourceType":"datasetVersion","datasetId":199387}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-14T07:42:42.409085Z","iopub.execute_input":"2024-11-14T07:42:42.409471Z","iopub.status.idle":"2024-11-14T07:42:43.622935Z","shell.execute_reply.started":"2024-11-14T07:42:42.409434Z","shell.execute_reply":"2024-11-14T07:42:43.621802Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/us-accidents/US_Accidents_March23.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"##### Import necessary libraries\nimport pandas as pd\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\n# Step 1: Load the dataset\ndf = pd.read_csv('/kaggle/input/us-accidents/US_Accidents_March23.csv')  # Update with your file path\nprint(\"Dataset loaded successfully. Columns are:\\n\", df.columns)\n\n# Step 2: Convert `Severity` to binary classification\n# Convert Severity to binary classification: 1 for Severe (3 and 4), 0 for Non-Severe (1 and 2)\ndf['Severity'] = df['Severity'].apply(lambda x: 1 if x >= 3 else 0)\nprint(\"Unique values in Severity column after conversion:\", df['Severity'].unique())\n\n# Step 3: Select relevant columns for the prediction task\ndf = df[['Severity', 'Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', 'Traffic_Signal']]\nprint(\"Selected columns:\\n\", df.head())\n\n# Step 4: Drop rows with missing values\ndf = df.dropna()\nprint(\"Data after dropping missing values. Shape:\", df.shape)\n\n# Step 5: Check initial class distribution to understand imbalance\nprint(\"Initial class distribution:\\n\", df['Severity'].value_counts())\n\n# Step 6: Undersample the majority class to 70% of its original size\n# Separate majority and minority classes\ndf_majority = df[df['Severity'] == 0]  # Non-Severe\ndf_minority = df[df['Severity'] == 1]  # Severe\n\n# Set the target majority size to 70% of its original size\ntarget_majority_size = int(0.85 * len(df_majority))\n\n# Perform undersampling on the majority class\ndf_majority_undersampled = resample(df_majority, \n                                    replace=False,                # Sample without replacement\n                                    n_samples=target_majority_size, # Reduce to 70% of majority class size\n                                    random_state=42)               # For reproducibility\n\n# Combine undersampled majority class with the minority class\ndf_balanced = pd.concat([df_majority_undersampled, df_minority])\n\n# Shuffle the balanced dataset\ndf_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Display new class distribution\nprint(\"Balanced class distribution:\\n\", df_balanced['Severity'].value_counts())\n\n# Step 7: Encode categorical columns to numeric values\n# Define features and target from the balanced dataset\nX = df_balanced.drop(columns=['Severity'])\ny = df_balanced['Severity']\n\n# Encode categorical columns\nlabel_encoder = LabelEncoder()\nfor column in X.columns:\n    if X[column].dtype == 'object':\n        X[column] = label_encoder.fit_transform(X[column])\n\nprint(\"Encoding completed. Sample data:\\n\", X.head())\n\n# Step 8: Split the data into training and testing sets with stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Display the class distribution in the training and testing sets\nprint(\"Training set class distribution:\\n\", y_train.value_counts())\nprint(\"Testing set class distribution:\\n\", y_test.value_counts())\n\n# Step 9: Train Multinomial, Bernoulli, and Gaussian Naive Bayes models\n\n# Multinomial Naive Bayes\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\ny_pred_mnb = mnb.predict(X_test)\nprint(\"\\nMultinomial Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_mnb))\nprint(\"Multinomial Naive Bayes Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mnb))\nprint(\"Multinomial Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_mnb))\n\n# Bernoulli Naive Bayes\nbnb = BernoulliNB()\nbnb.fit(X_train, y_train)\ny_pred_bnb = bnb.predict(X_test)\nprint(\"\\nBernoulli Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_bnb))\nprint(\"Bernoulli Naive Bayes Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_bnb))\nprint(\"Bernoulli Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_bnb))\n\n# Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred_gnb = gnb.predict(X_test)\nprint(\"\\nGaussian Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_gnb))\nprint(\"Gaussian Naive Bayes Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gnb))\nprint(\"Gaussian Naive Bayes Classification Report:\\n\", classification_report(y_test, y_pred_gnb))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T07:57:03.128201Z","iopub.execute_input":"2024-11-14T07:57:03.129126Z","iopub.status.idle":"2024-11-14T07:59:22.400711Z","shell.execute_reply.started":"2024-11-14T07:57:03.129070Z","shell.execute_reply":"2024-11-14T07:59:22.399139Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully. Columns are:\n Index(['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat',\n       'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description',\n       'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone',\n       'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)',\n       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction',\n       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity',\n       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n       'Astronomical_Twilight'],\n      dtype='object')\nUnique values in Severity column after conversion: [1 0]\nSelected columns:\n    Severity Weather_Condition Sunrise_Sunset Civil_Twilight  Traffic_Signal\n0         1        Light Rain          Night          Night           False\n1         0        Light Rain          Night          Night           False\n2         0          Overcast          Night          Night            True\n3         1     Mostly Cloudy          Night            Day           False\n4         0     Mostly Cloudy            Day            Day            True\nData after dropping missing values. Shape: (7534613, 5)\nInitial class distribution:\n Severity\n0    6070980\n1    1463633\nName: count, dtype: int64\nBalanced class distribution:\n Severity\n0    5160333\n1    1463633\nName: count, dtype: int64\nEncoding completed. Sample data:\n    Weather_Condition  Sunrise_Sunset  Civil_Twilight  Traffic_Signal\n0                  6               1               1           False\n1                 15               0               0           False\n2                 61               1               1           False\n3                 87               0               0           False\n4                 84               0               0           False\nTraining set class distribution:\n Severity\n0    3612233\n1    1024543\nName: count, dtype: int64\nTesting set class distribution:\n Severity\n0    1548100\n1     439090\nName: count, dtype: int64\n\nMultinomial Naive Bayes Accuracy: 0.7790397495961634\nMultinomial Naive Bayes Confusion Matrix:\n [[1548100       0]\n [ 439090       0]]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Multinomial Naive Bayes Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.78      1.00      0.88   1548100\n           1       0.00      0.00      0.00    439090\n\n    accuracy                           0.78   1987190\n   macro avg       0.39      0.50      0.44   1987190\nweighted avg       0.61      0.78      0.68   1987190\n\n\nBernoulli Naive Bayes Accuracy: 0.7790397495961634\nBernoulli Naive Bayes Confusion Matrix:\n [[1548100       0]\n [ 439090       0]]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Bernoulli Naive Bayes Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.78      1.00      0.88   1548100\n           1       0.00      0.00      0.00    439090\n\n    accuracy                           0.78   1987190\n   macro avg       0.39      0.50      0.44   1987190\nweighted avg       0.61      0.78      0.68   1987190\n\n\nGaussian Naive Bayes Accuracy: 0.7784786557903371\nGaussian Naive Bayes Confusion Matrix:\n [[1546700    1400]\n [ 438805     285]]\nGaussian Naive Bayes Classification Report:\n               precision    recall  f1-score   support\n\n           0       0.78      1.00      0.88   1548100\n           1       0.17      0.00      0.00    439090\n\n    accuracy                           0.78   1987190\n   macro avg       0.47      0.50      0.44   1987190\nweighted avg       0.64      0.78      0.68   1987190\n\n","output_type":"stream"}],"execution_count":6}]}